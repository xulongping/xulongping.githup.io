<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>深度学习Batch size调参</title>
</head>
<body><center><font size="40">深度学习Batch Size如何调参</font></center>
<h1 id='1浅析batch-size影响'>1.浅析batch size影响</h1>
<p>		深度学习中batch size的大小对训练过程的影响是什么样的？</p>
<p>		不考虑 batch normalization的情况下，batch size的大小决定了深度学习训练过程中的<strong>完成每个epoch所需的时间和每次迭代（iteration）之间梯度的平滑程度。</strong></p>
<p>		对于一个大小为N的训练集，如果每个epoch中mini-batch的采样方法采用最常规的N个样本每个都采样一次，设mini-batch大小为b，那么每个epoch所需的迭代次数(正向+反向)为 <mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.416ex" height="2.782ex" role="img" focusable="false" viewBox="0 -877 1067.9 1229.7" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.798ex;"><defs><path id="MJX-18-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path id="MJX-18-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mi" transform="translate(220,394) scale(0.707)"><use data-c="1D441" xlink:href="#MJX-18-TEX-I-1D441"></use></g><g data-mml-node="mi" transform="translate(382.3,-345) scale(0.707)"><use data-c="1D44F" xlink:href="#MJX-18-TEX-I-1D44F"></use></g><rect width="827.9" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container><script type="math/tex">\frac{N}{b}</script> , <strong>因此完成每个epoch所需的时间大致也随着迭代次数的增加而增加。</strong></p>
<p>		由于目前主流深度学习框架处理mini-batch的反向传播时，<strong>默认都是先将每个mini-batch中每个instance得到的loss平均化之后再反求梯度</strong>，也就是说每次反向传播的梯度是对mini-batch中每个instance的梯度平均之后的结果，所以b的大小决定了相邻迭代之间的梯度平滑程度，<strong>b太小，相邻mini-batch间的差异相对过大，那么相邻两次迭代的梯度震荡情况会比较严重，不利于收敛；b越大，相邻mini-batch间的差异相对越小，虽然梯度震荡情况会比较小，一定程度上利于模型收敛，但如果b极端大，相邻mini-batch间的差异过小，相邻两个mini-batch的梯度没有区别了，整个训练过程就是沿着一个方向蹭蹭蹭往下走，很容易陷入到局部最小值出不来。</strong></p>
<p>		总结下来：<strong>batch size过小，话费时间多，同时梯度震荡严重，不利于收敛；batch size过大，不同batch的梯度方向没有任何变化，容易陷入局部极小值。</strong></p>
<h2 id='11-batch-size与learning-rate的关系'>1.1 Batch size与learning rate的关系</h2>
<p>		较小的batch size要设置小lr，batch size越小，相邻iter之间的loss震荡就越厉害，异常值对结果造成巨大扰动。较大的batch size，要设置大一点的lr，原因是大batch size每次迭代的梯度方向相对固定，大lr可以加速其收敛过程。</p>
<h1 id='2实验证明'>2.实验证明</h1>
<p>		用MINST做一下实验，超参数：SGD(lr = 0.02, momentum=0.5)，看一下不同batch size之间的区别：</p>
<h2 id='21-迭代速度'>2.1 迭代速度</h2>
<p><img src="../image/tuning_paraneter/batch_size-迭代速度.png" referrerpolicy="no-referrer" alt="batch_size-迭代速度"></p>
<p>		表中 Epoch Time是在此batch size下完成一个epoch所需的所有时间，包括加载数据和计算的时间，Epoch Computation Time抛去了加载数据所需的时间。</p>
<p> 		其实纯粹cuda计算的角度来看，完成每个iter的时间大batch和小batch区别并不大，这可能是因为<strong>本次实验中，反向传播的时间消耗要比正向传播大得多，所以batch size的大小对每个iter所需的时间影响不明显，未来将在大一点的数据库和更复杂的模型上做一下实验。</strong>（因为反向的过程取决于模型的复杂度，与batchsize的大小关系不大，而正向则同时取决于模型的复杂度和batch size的大小。而本次实验中反向的过程要比正向的过程时间消耗大得多，所以batch size的大小对完成每个iter所需的耗时影响不大。）</p>
<p>		完成每个epoch运算的所需的全部时间主要卡在：1. load数据的时间，2. 每个epoch的iter数量。 因此对于每个epoch，不管是纯计算时间还是全部时间，大体上还是大batch能够更节约时间一点，但随着batch增大，iter次数减小，完成每个epoch的时间更取决于加载数据所需的时间，此时也不见得大batch能带来多少的速度增益了。</p>
<h2 id='22-梯度平滑度'>2.2 梯度平滑度</h2>
<p>	不同batch size下的梯度平滑度，选取每个batch size下前1000个iter的loss，来看一下loss的震荡情况：</p>
<p><img src="../image/tuning_paraneter/batch_size-梯度平滑度1.jpg" alt="batch_size-梯度平滑度1" style="zoom:40%;" /></p>
<p>如果感觉这张图片不太好看，可以看一下这张图：</p>
<p><img src="../image/tuning_paraneter/batch_size-梯度平滑度2.jpg" alt="batch_size-梯度平滑度2" style="zoom:40%;" /></p>
<p>		由于现在绝大多数的框架在进行mini-batch的反向传播的时候，默认都是将batch中每个instance的loss平均化之后在进行反向传播，所以相对大一点的batch size能够防止loss震荡的情况发生。从这两张图中可以看出batch size越小，相邻iter之间的loss震荡就越厉害，相应的，反传回去的梯度的变化也就越大，也就越不利于收敛。同时很有意思的一个现象，batch size为1的时候，loss到后期会发生爆炸，这主要是lr=0.02设置太大，所以某个异常值的出现会严重扰动到训练过程。<strong>这也是为什么对于较小的batchsize，要设置小lr的原因之一，避免异常值对结果造成的扰巨大扰动。而对于较大的batchsize，要设置大一点的lr，原因则是大batch每次迭代的梯度方向相对固定，大lr可以加速其收敛过程。</strong></p>
<h2 id='23-收敛速度'>2.3 收敛速度</h2>
<p>		在衡量不同batch size的优劣这一点伤，我选用衡量不同batch size在同样参数下的收敛速度快慢的方法。</p>
<p>		下表中可以看出，在minst数据集上，从整体时间消耗上来看（考虑了加载数据所需的时间），同样的参数策略下 (lr = 0.02, momentum=0.5 ），要模型收敛到accuracy在98左右，batch size在 6 - 60 这个量级能够花费最少的时间，而batch size为1的时候，收敛不到98；batch size过大的时候，因为模型收敛快慢取决于梯度方向和更新次数，所以大batch尽管梯度方向更为稳定，但要达到98的accuracy所需的更新次数并没有量级上的减少，所以也就需要花费更多的时间，当然这种情况下可以配合一些调参策略比如warmup LR，衰减LR等等之类的在一定程度上进行解决（这个先暂且按下不表），但也不会有本质上的改善。</p>
<p>		不过单纯从计算时间上来看，大batch还是可以很明显地节约所需的计算时间的，原因前面讲过了，主要因为本次实验中纯计算时间中，反向占的时间比重远大于正向。</p>
<p><img src="../image/tuning_paraneter/batch_size-收敛速度.png" referrerpolicy="no-referrer" alt="batch_size-收敛速度"></p>
<p>		<strong>直接比较不同batch size下的绝对收敛精度来衡量batch size的好坏是没有太大意义的，因为不同的batch size要配合不同的调参策略用才能达到其最佳效果，而要想在每个batch size下都找到合适的调参策略那可太难了，所以用这种方法来决定batch size未免有点武断。</strong></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</body>
</html>