<center><font size='40'>优化器Optimizer</font></center>

## 1.什么是优化器

### 1.1 解释

​		**优化器是在深度学习方向传播过程中，指引损失函数（目标函数）的各个参数往正确的方向更新合适的大小，使得更新后的各个参数让损失函数（目标函数）值不断逼近全局最小。**

​		优化问题可以看做是我们站在山上的某个位置（当前的参数信息），想要以最佳的路线去到山下（最优点）。首先，直观的方法就是环顾四周，找到下山最快的方向走一步，然后再次环顾四周，找到最快的方向，直到下山——这样的方法便是朴素的梯度下降。当前的海拔是我们的目标函数值，而我们在每一步找到的方向便是函数梯度的反向（梯度是函数上升最快的方向，所以梯度的反方向就是函数下降最快的方向）。

​		事实上，使用梯度下降进行优化，是几乎所有优化器的核心思想。当我们下山时，两个方面是我们最关心的：

- 首先是优化方向

  决定前进的方向是否正确，在优化器中反映为梯度或动量

- 其次是步长

  决定每一步迈多远，在优化器中反映为学习率

  所有的优化器都在关注这两个方面，但同时也有一些其他问题，比如应该在哪里出发、路线错误如何处理......这是一些最新的优化器关注的方向。

### 1.2 公式和定义

​		待优化参数：$\omega$，目标函数：f(x)，初始学习率：$\alpha$，迭代epoch：t

​		参数更新步骤如下：

1. 计算目标函数关于当前参数的梯度

   ​		$g_t = \nabla f(w_t)$

2. 根据历史梯度计算一阶动量和二阶动量

   ​		$m_t = \phi (g_1, g_2, ..., g_t);  V_t = \sum_{i=0}^t x_i^2$

3. 计算当前时刻的下降梯度

   ​		$\eta_t = \alpha * m_t / \sqrt{V_t}$

4. 根据下降梯度进行更新参数

   ​		$w_{t+1} = w_t - \eta_t$

   步骤3、4对于各个算法都是一致的，主要的差别就体现在步骤1、2上。

## 2.有哪些优化器

​		机器学习中，有很多有哈方法来试图寻找模型的最优解。比如神经网络中可以采用最基本的梯度下降法。

​		**梯度下降法**是最基本的一类优化器，目前主要分为三种梯度下降法：**标准梯度下降法（GD，Gradient Descent）**，**随机梯度下降法（SGD，Stochastic Gradient Descent）**及**批量梯度下降法（BGD，Batch Gradient Descent）**。

​		**动量优化方法**是在梯度下降法的基础上进行改变，具有加速梯度下降的作用。一般有标准动量优化方法**Momentum、NAG（Nesterov accelerated gradient）**动量优化方法。

​		**自适应学习率优化**算法针对与机器学习模型的学习率，传统的优化算法要么将学习率设置为常数要么根据训练次数调节学习率。极大忽视了学习率其他变化的可能性。然而，学习率对模型的性能有着显著的影响，因此需要采取一些策略来想办法更新学习率，从而提升训练速度。目前的自适应学习率优化主要有：**AdaGrad算法**、**RMSProp算法**，**Adam算法**以及**AdaDelta算法**。

### 2.1 梯度下降法

#### 2.1.1 标准梯度下降法（GD）

​		假设要学习训练的模型参数为W，代价函数为J(W)，则代价函数关于模型参数的偏导数即相关梯度为ΔJ(W)，学习率为ηt，则使用梯度下降法更新参数为：
​															$W_{t+1} = W_t - η_tΔJ(W_t)$
​		其中，$W_t$表示t时刻的模型参数。

​		从表达式来看，模型参数的更新调整，与代价函数关于模型参数的梯度有关，即沿着梯度的方向不断减小模型参数，从而最小化代价函数。
​		基本策略可以理解为”在有限视距内寻找最快路径下山“，因此每走一步，参考当前位置最陡的方向(即梯度)进而迈出下一步。可以形象的表示为：

![optimizer-标准梯度下降法](../image/tuning_paraneter/optimizer-标准梯度下降法.png)

​	**评价**：标准梯度下降法主要有两个缺点：

- **训练速度慢**：每走一步都要计算调整下一步的方向，下山的速度变慢。在应用于大型数据集中，每输入一个样本都要更新一次参数，且每次迭代都要遍历所有的样本。会使得训练过程及其缓慢，需要花费很长时间才能得到收敛解。
- **容易陷入局部最优解**：由于是在有限距内寻找下山的反向。当陷入平坦的洼地，会误以为达到了山地的最低点，从而不会继续往下走。所谓的局部最优解就是鞍点。落入鞍点，梯度为0，使得模型参数不在继续更新。

#### 2.1.2 批量梯度下降法（BGD）

​		假设批量训练样本总数为n，每次输入和输出的样本分别为$X^{(i)},Y^{(i)}$，模型参数为W，代价函数为J(W)，每输入一个样本i代价函数关于W的梯度为$ΔJ_i(W_t,X^{(i)},Y^{(i)})$，学习率为$η_t$，则使用批量梯度下降法更新参数表达式为：
​												$W_{t+1}= W_t − η_t\sum_{i=1}^n ΔJ_i(W_t,X^{(i)},Y^{(i)})$

​		其中，$W_t$表示t时刻的模型参数。

​		从表达式来看，模型参数的调整更新与全部输入样本的代价函数的和（即批量/全局误差）有关。即每次权值调整发生在批量样本输入之后，而不是每输入一个样本就更新一次模型参数。这样就会大大加快训练速度。
​		基本策略可以理解为，在下山之前掌握了附近的地势情况，选择总体平均梯度最小的方向下山。

**评价：**

- 批量梯度下降法比标准梯度下降法训练时间短，且每次下降的方向都很正确。

#### 2.1.3 随机梯度下降法（SGD）

​		对比批量梯度下降法，假设从一批训练样本n中随机选取一个样本$i_s$。模型参数为W，代价函数为J(W)，梯度为ΔJ(W)，学习率为$η_t$，则使用随机梯度下降法更新参数表达式为：
​                                                   $W_{t+1}=W_t − η_t g_t$
​		其中，$g_t=ΔJ_{i_s}(W_t;X^{(i_s)};X^{(i_s)}),  i_s∈{1,2,...,n}$表示随机选择的一个梯度方向，$W_t$表示t时刻的模型参数。

​		$E(g_t)=ΔJ(W_t)$，这里虽然引入了随机性和噪声，但期望仍然等于正确的梯度下降。
​		基本策略可以理解为随机梯度下降像是一个盲人下山，不用每走一步计算一次梯度，但是他总能下到山底，只不过过程会显得扭扭曲曲。

![optimizer-随机梯度下降法](../image/tuning_paraneter/optimizer-随机梯度下降法.jpg)

评价：

- 优点
  - 虽然SGD需要走很多步的样子，但是对梯度的要求很低（计算梯度快）。而对于引入噪声，大量的理论和实践工作证明，只要噪声不是特别大，SGD都能很好地收敛
  - 应用大型数据集时，训练速度很快。比如每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。相比于标准梯度下降法的遍历全部样本，每输入一个样本更新一次参数，要快得多。
- 缺点
  - SGD在随机选择梯度的同时会引入噪声，使得权值更新的方向不一定正确
  - SGD也没能单独克服局部最优解的问题

### 2.2 动量优化法

#### 2.2.1 SGD with Momentum

**思想：**

​		为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。

**算法描述：**

​		在SGD基础上引入一阶动量：

​					$m_t = \beta_1 * m_{t-1} + (1 - \beta_1) * g_t$

​		SGD-M参数更新公式如下，其中$\alpha$是学习率， $g_t$是当前参数的梯度

​					$W_{t+1} = W_t - \alpha * m_t = W_t - \alpha * (\beta_1 * m_{t-1} + (1-\beta_1) * g_t)$

​		一阶动量是各个时刻梯度方向的指数移动平均值，也就是说， t时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。 $\beta_1$的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。想象高速公路上汽车转弯，在高速向前的同时略微偏向，急转弯可是要出事的。

​		动量主要解决SGD的两个问题：一是随机梯度的方法（引入的噪声）；二是Hessian矩阵病态问题（可以理解为SGD在收敛过程中和正确梯度相比来回摆动比较大的问题）。
​		**理解策略为：由于当前权值的改变会受到上一次权值改变的影响，类似于小球向下滚动的时候带上了惯性。这样可以加快小球向下滚动的速度。**

**评价：**

​		因为加入了动量因素，SGD-M缓解了SGD在局部最优点梯度为0，无法持续更新的问题和振荡幅度过大的问题，但是并没有完全解决，当局部沟壑比较深，动量加持用完了，依然会困在局部最优里来回震荡。

​		

#### 2.2.2 NAG

​		牛顿加速梯度（NAG, Nesterov accelerated gradient）算法，是Momentum动量算法的变种。更新模型参数表达式如下：

​											$\begin{cases}v_t = αv_{t−1} + η_t ΔJ(W_t - αv_{t-1}) \\ W_{t+1}=W_t − v_t\end{cases}$

​		其中，$v_t$表示t时刻积攒的加速度；α表示动力的大小；$η_t$表示学习率；$W_t$表示t时刻的模型参数，$ΔJ(W_t−αv_{t−1})$表示代价函数关于$W_t$的梯度。

​		Nesterov动量梯度的计算在模型参数施加当前速度之后，因此可以理解为往标准动量中添加了一个校正因子。

​		理解策略：在Momentun中小球会盲目地跟从下坡的梯度，容易发生错误。所以需要一个更聪明的小球，能提前知道它要去哪里，还要知道走到坡底的时候速度慢下来而不是又冲上另一个坡。计算$W_t−αv_{t−1}$
可以表示小球下一个位置大概在哪里。从而可以提前知道下一个位置的梯度，然后使用到当前位置来更新参数。
​		在凸批量梯度的情况下，Nesterov动量将额外误差收敛率从O(1/k)(k步后)
改进到$O(1/k^2)$。然而，在随机梯度情况下，Nesterov动量对收敛率的作用却不是很大。

### 2.3 自适应学习率优化算法

#### 2.3.1 AdaGrad算法

​		SGD系列的都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。

**思想：**

​		AdaGrad算法，独立地适应所有模型参数的学习率，缩放每个参数反比与其所有梯度历史平均值总和的平方根。具有代价函数最大梯度的参数相应地有个快速下降的学习率，而具有小梯度的参数在学习率上有相对较小的下降。

**算法描述：**

​		怎样去度量历史更新频率呢？那就是二阶动量——该维度上，记录到目前为止所有梯度值的平方和：

​						$V_t = \sum_{\tau=1}^tg_\tau^2$

​		我们再回顾一下步骤3中的下降梯度：

​						$\eta_t = \alpha * m_t/\sqrt{V_t}$			

​		AdaGrad参数更新公式如下，其中$\alpha$是学习率，$g_t$是当前参数的梯度

​						$W_{t+1} = W_t - \alpha * m_t/\sqrt{V_t} = W_t - \alpha * m_t/\sqrt{\sum_{\tau=1}^tg_\tau^2}$			

​		可以看出，此时实质上的学习率由$\alpha$变成了$\alpha/\sqrt{V_t}$ 。一般避免分母为0，会在分母上加一个小的平滑项。因此$\sqrt{V_t}$是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率越小。									

**评价：**

- 优点
  - 在稀疏数据场景下表现非常好
  - 此前的SGD及其变体的优化器主要聚焦在优化梯度前进的方向上，而AdaGrade首次使用二阶动量来关注学习率（步长），开启自适应学习率算法的里程
- 缺点
  - 因为$\sqrt{V_t}$是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。

#### 2.3.2 RMSProp算法

**思想：**

​		RMSProp算法修改了AdaGrad的梯度积累为指数加权的移动平均，使得其在非凸设定下效果更好。

**算法描述：**

**评价：**

- 优点
  - RMSProp借鉴了Adagrad的思想，观察表达式，分母为$\sqrt{E[g^2]_t + ϵ}$。由于取了个加权平均，避免了学习率越来越低的的问题，而且能自适应地调节学习率。
  - RMSProp算法在经验上已经被证明是一种有效且实用的深度神经网络优化算法。目前它是深度学习从业者经常采用的优化方法之一。

#### 2.3.3 AdaDelta算法

**思想：**

​		由于AdaGrad单调递减的学习率变化过于激进，考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中的Delta的来历。

**算法描述：**

​		指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累计动量：

​					$V_t = \beta_2 * V_{t-1} + (1-\beta_2) g_t^2$

​		AdaDelta参数更新公式如下，其中$\alpha$是学习率，$g_t$是当前参数的梯度

​					$W_{t+1} = W_t - \alpha * m_t / \sqrt{V_t}$

​							$=W_t - \alpha * m_t/\sqrt{\beta_2 * V_{t-1} + (1 - \beta_2)g_t^2}$

**评价：**

- 优点
  - 避免了二阶动能持续累积、导致训练过程提前结束的问题

#### 2.3.4 Adam算法

​	Adam是前述方法的集大成者。SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum

**思想：**

​		Adam中动量直接并入了梯度一阶矩（指数加权）的估计。其次，相比于缺少修正因子导致二阶矩估计可能在训练初期具有很高偏置的RMSProp，Adam包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩估计。

**算法描述：**

​		SGD的一阶动量：

​				$m_t = \beta_1 * m_{t-1} + (1 - \beta_1) * g_t$

​		加上AdaDelta的二阶动量：

​				$V_t = \beta_2 * V_{t-1} + (1 - \beta_2) g_t^2$

​		Adam参数更新公式如下，其中$\alpha$是学习率，$g_t$是当前参数的梯度

​				$W_{t+1} = W_t - \alpha * m_t/\sqrt{V_t}$

​						  $= W_t - \alpha * (\beta_1 * m_{t -1} + (1 - \beta_1) * g_t) / \sqrt{\beta_2 * V_{t-1} + (1 - \beta_2)g_t^2}$

​		优化算法里最常见的两个超参数$\beta_1, \beta_2$，前者控制一阶动量，后者控制二阶动量。

**评价：**

- 优点

  - 通过一阶动量和二阶动量，有效控制学习率步长和梯度方向，防止梯度的振荡和鞍点的静止

- 缺点

  - 可能不收敛

    二阶动量是固定窗口内的积累，随着时间窗口的变化，遇到的数据可能发生巨变，使得$V_t$可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。

    修正的方法，由于Adam中的学习率主要是由二阶动量控制，为了保证算法的收敛，可以对二阶动量的变化进行控制，避免上下波动。

    ​		$V_t = max(\beta_2 * V_{t-1} + (1 - \beta_2)g_t^2, V_{t-1})$

    通过这样的修改，保证了$||V_t|| >= ||V_{t-1}||$， 从而使得学习率单调递减。

  - 可能错过全局最优解

    自适应学习算法可能会前期出现的特征过拟合，后期才出现的特征很难纠正前期的拟合效果。后期Adam的学习率太低，影响了有效的收敛。

### 2.4 总结

​		SGD参数更新公式如下，其中 $\alpha$是学习率，$g_t$是当前参数的梯度：

​				 $W_{t+1} = W_t - \alpha * g_t$

​		优化器千变万化，五花八门，其实只要还是步长$\alpha$和梯度方向$g_t$两个层面进行改进，都是SGD带不同的learning rate scheduler。

## 3.优化算法的选择与使用策略

​		优化算法的常用tricks：

1. 首先，各大算法孰优孰劣并无定论。如果是刚入门，优先考虑SGD+Nesterov Momentum或者Adam。
2. 选择你熟悉的算法——这样你可以更加熟练地利用你的经验进行调参。
3. 充分了解你的数据——如果模型是非常稀疏的，那么优先考虑自适应学习率的算法。
4. 根据你的需求来选择——在模型设计实验过程中，要快速验证新模型的效果，可以先用Adam进行快速实验优化；在模型上线或者结果发布前，可以用精调的SGD进行模型的极致优化。
5. 先用小数据集进行实验。有论文研究指出，随机梯度下降算法的收敛速度和数据集的大小的关系不大。因此可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，并通过参数搜索来寻找最优的训练参数。
6. 考虑不同算法的组合。先用Adam进行快速下降，而后再换到SGD进行充分的调优。切换策略可以参考本文介绍的方法。
7. 数据集一定要充分的打散（shuffle）。这样在使用自适应学习率算法的时候，可以避免某些特征集中出现，而导致的有时学习过度、有时学习不足，使得下降方向出现偏差的问题。
8. 训练过程中持续监控训练数据和验证数据上的目标函数值以及精度或者AUC等指标的变化情况。对训练数据的监控是要保证模型进行了充分的训练——下降方向正确，且学习率足够高；对验证数据的监控是为了避免出现过拟合。
9. 制定一个合适的学习率衰减策略。可以使用定期衰减策略，比如每过多少个epoch就衰减一次；或者利用精度或者AUC等性能指标来监控，当测试集上的指标不变或者下跌时，就降低学习率。

































